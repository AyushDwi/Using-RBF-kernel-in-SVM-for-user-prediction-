import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.pipeline import make_pipeline
import seaborn as sns

# 1. Importing data and creating a dataframe
df = pd.read_csv("https://github.com/AyushDwi/dataset/raw/main/user-data.csv")
print(df.head())
print(df.dtypes)

# 2. Extracting independent and dependent variables
X = df.iloc[:, [2, 3]].values  # Age and Estimated Salary (independent)
y = df.iloc[:, 4].values       # Purchased (dependent)

print(X[:5])
print(y[:5])

# 3. Feature scaling (using StandardScaler)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Use StandardScaler for scaling

# 4. Polynomial Features (degree=2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_scaled)

# 5. Stratified K-Fold Cross-Validation (10 folds)
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)

# 6. Initialize lists to store true and predicted values for each fold
y_true = []
y_pred = []

# Initialize the SVM classifier with RBF kernel
model = SVC(kernel="rbf", class_weight="balanced", random_state=0)

# 7. Perform Stratified K-Fold Cross-Validation
for train_index, test_index in skf.split(X_poly, y):
    X_train, X_test = X_poly[train_index], X_poly[test_index]
    y_train, y_test = y[train_index], y[test_index]

    # Train the model
    model.fit(X_train, y_train)

    # Predict the test set
    y_pred_single = model.predict(X_test)

    # Store the true and predicted values
    y_true.extend(y_test)
    y_pred.extend(y_pred_single)

# Convert lists to numpy arrays for evaluation
y_true = np.array(y_true)
y_pred = np.array(y_pred)

# 8. Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
print(cm)

sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.title("Confusion Matrix (Stratified K-Fold Cross-Validation)")
plt.show()

# 9. Accuracy
acc = accuracy_score(y_true, y_pred)
print("Accuracy:", acc)

# 10. ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_true, y_pred)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')  # Random predictions curve
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# 11. Hyperparameter Tuning with GridSearchCV
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}
grid_search = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid, cv=skf, scoring='accuracy')
grid_search.fit(X_poly, y)

print("Best parameters:", grid_search.best_params_)

# 12. Predict on a new test sample (example: Age = 40, Estimated Salary = 50000)
new_data = np.array([[40, 50000]])
new_data_scaled = scaler.transform(new_data)
new_data_poly = poly.transform(new_data_scaled)
new_prediction = grid_search.best_estimator_.predict(new_data_poly)

if new_prediction[0] == 1:
    print("Prediction: The user is likely to purchase the product.")
else:
    print("Prediction: The user is not likely to purchase the product.")
